diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
index 6b3045e..804d236 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
@@ -33,6 +33,7 @@
 import java.util.Queue;
 import java.util.Set;
 
+import com.google.common.collect.Sets;
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -557,12 +558,23 @@ private String getExplainOutput(BaseSemanticAnalyzer sem, QueryPlan plan,
    */
   public static void doAuthorization(BaseSemanticAnalyzer sem, String command)
       throws HiveException, AuthorizationException {
-    HashSet<ReadEntity> inputs = sem.getInputs();
-    HashSet<WriteEntity> outputs = sem.getOutputs();
+    Set<ReadEntity> additionalInputs = new HashSet<ReadEntity>();
+    Set<WriteEntity> additionalOutputs = new HashSet<WriteEntity>();
+
     SessionState ss = SessionState.get();
     HiveOperation op = ss.getHiveOperation();
     Hive db = sem.getDb();
 
+    for (Table inputTable  : getMissingParentTablesForPartitionEntities(sem.getInputs())){
+      additionalInputs.add(new ReadEntity(inputTable));
+    }
+    for (Table outputTable  : getMissingParentTablesForPartitionEntities(sem.getOutputs())){
+      additionalOutputs.add(new WriteEntity(outputTable, WriteEntity.WriteType.DDL_NO_LOCK));
+    }
+
+    Set<ReadEntity> inputs = Sets.union(sem.getInputs(), additionalInputs);
+    Set<WriteEntity> outputs = Sets.union(sem.getOutputs(), additionalOutputs);
+
     if (ss.isAuthorizationModeV2()) {
       // get mapping of tables to columns used
       ColumnAccessInfo colAccessInfo = sem.getColumnAccessInfo();
@@ -708,6 +720,31 @@ public static void doAuthorization(BaseSemanticAnalyzer sem, String command)
     }
   }
 
+  private static Set<Table> getMissingParentTablesForPartitionEntities(Set<? extends Entity> entities) {
+    Set<Table> expectedTables = new HashSet<Table>();
+    Set<Table> foundTables = new HashSet<Table>();
+    for (Entity e : entities) {
+      switch (e.getType()) {
+        case PARTITION:
+          expectedTables.add(e.getTable());
+          break;
+        case TABLE:
+          foundTables.add(e.getTable());
+          break;
+        default:
+          // don't care about other types for this validation
+          break;
+      }
+    }
+    Set<Table> tablesNotFound = new HashSet<Table>();
+    for (Table expectedTable : expectedTables) {
+      if (!foundTables.contains(expectedTable)) {
+        tablesNotFound.add(expectedTable);
+      }
+    }
+    return tablesNotFound;
+  }
+
   private static void getTablePartitionUsedColumns(HiveOperation op, BaseSemanticAnalyzer sem,
       Map<Table, List<String>> tab2Cols, Map<Partition, List<String>> part2Cols,
       Map<String, Boolean> tableUsePartLevelAuth) throws HiveException {
@@ -763,8 +800,8 @@ private static void getTablePartitionUsedColumns(HiveOperation op, BaseSemanticA
 
   }
 
-  private static void doAuthorizationV2(SessionState ss, HiveOperation op, HashSet<ReadEntity> inputs,
-      HashSet<WriteEntity> outputs, String command, Map<String, List<String>> tab2cols,
+  private static void doAuthorizationV2(SessionState ss, HiveOperation op, Set<ReadEntity> inputs,
+      Set<WriteEntity> outputs, String command, Map<String, List<String>> tab2cols,
       Map<String, List<String>> updateTab2Cols) throws HiveException {
 
     /* comment for reviewers -> updateTab2Cols needed to be separate from tab2cols because if I
@@ -784,7 +821,7 @@ private static void doAuthorizationV2(SessionState ss, HiveOperation op, HashSet
   }
 
   private static List<HivePrivilegeObject> getHivePrivObjects(
-      HashSet<? extends Entity> privObjects, Map<String, List<String>> tableName2Cols) {
+      Set<? extends Entity> privObjects, Map<String, List<String>> tableName2Cols) {
     List<HivePrivilegeObject> hivePrivobjs = new ArrayList<HivePrivilegeObject>();
     if(privObjects == null){
       return hivePrivobjs;
